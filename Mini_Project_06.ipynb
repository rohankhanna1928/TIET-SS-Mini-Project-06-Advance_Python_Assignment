{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTdj484Cai2zk5LvjQC4TD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohankhanna1928/Mini-Project-06/blob/main/Mini_Project_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part I: Process Automation**"
      ],
      "metadata": {
        "id": "2VAi030TUvik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. Create a file that contains 1000 lines of random strings**"
      ],
      "metadata": {
        "id": "F1fsVvdhU0Qo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDYrunIyUsZq"
      },
      "outputs": [],
      "source": [
        "import random as r\n",
        "import string as s\n",
        "\n",
        "s = s.ascii_letters + s.digits\n",
        "with open('random strings.txt', 'w') as file:\n",
        "  for i in range(1000):\n",
        "    S = \"\"\n",
        "    for j in range(5):\n",
        "      S += r.choice(s)\n",
        "    file.write(S + '\\n')\n",
        "print(\"File saved as 'random strings.txt'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. Create a file that contains multiple lines of random strings and file size must be 5 MB.**"
      ],
      "metadata": {
        "id": "BJN2DgP_WWTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as r\n",
        "import string as s\n",
        "\n",
        "target_size = 5 * 1024 * 1024\n",
        "output_file = 'random 5MB strings.txt'\n",
        "s = s.ascii_letters + s.digits\n",
        "line_length = 1000\n",
        "\n",
        "with open(output_file, 'w') as file:\n",
        "    total_written = 0\n",
        "    while total_written < target_size:\n",
        "        line = ''.join(r.choices(s, k=line_length)) + '\\n'\n",
        "        file.write(line)\n",
        "        total_written += len(line)\n",
        "\n",
        "print(\"File saved as 'random 5MB strings.txt'\")"
      ],
      "metadata": {
        "id": "mrrOESJUWa6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3. Create 10 files that contains multiple lines of random strings and file size of each file must be 5 MB.**"
      ],
      "metadata": {
        "id": "mgalKrPtZ98w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as r\n",
        "import string as s\n",
        "import os\n",
        "\n",
        "num_files = 10\n",
        "target_size = 5 * 1024 * 1024  # 5 MB\n",
        "line_length = 1000\n",
        "s = s.ascii_letters + s.digits\n",
        "\n",
        "for i in range(1, num_files + 1):\n",
        "    filename = f'random_file_{i}.txt'\n",
        "\n",
        "    with open(filename, 'w') as file:\n",
        "        total_written = 0\n",
        "        while total_written < target_size:\n",
        "            line = ''.join(r.choices(s, k=line_length)) + '\\n'\n",
        "            file.write(line)\n",
        "            total_written += len(line)\n",
        "\n",
        "    print(f'Created: {filename} ({os.path.getsize(filename)} bytes)')"
      ],
      "metadata": {
        "id": "6no2RZ5ZaBF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4. Create 5 files of size 1GB, 2GB, 3GB, 4GB and 5GB; file contains multiple lines of random strings.**"
      ],
      "metadata": {
        "id": "sdLTI-1dcmUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as r\n",
        "import string as s\n",
        "import os\n",
        "\n",
        "s = s.ascii_letters + s.digits\n",
        "\n",
        "file_sizes_gb = [1, 2, 3, 4, 5]\n",
        "line_length = 10000\n",
        "\n",
        "file_sizes_bytes = [gb * 1024 * 1024 * 1024 for gb in file_sizes_gb]\n",
        "\n",
        "for i, size in enumerate(file_sizes_bytes, start=1):\n",
        "    filename = f'random_file_{i}GB.txt'\n",
        "    with open(filename, 'w') as file:\n",
        "        total_written = 0\n",
        "        while total_written < size:\n",
        "            line = ''.join(r.choices(s, k=line_length)) + '\\n'\n",
        "            file.write(line)\n",
        "            total_written += len(line)\n",
        "    print(f'Created: {filename} ({os.path.getsize(filename)} gb)')"
      ],
      "metadata": {
        "id": "rTMYB6StcsZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5. Convert all the files of Q4 into upper case one by one.**"
      ],
      "metadata": {
        "id": "HF30B0WnevYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_sizes_gb = [1, 2, 3, 4, 5]\n",
        "\n",
        "for i, gb in enumerate(file_sizes_gb, start=1):\n",
        "    input_filename = f'random_file_{i}GB.txt'\n",
        "    output_filename = f'random_file_{i}GB_upper.txt'\n",
        "\n",
        "    with open(input_filename, 'r') as infile, open(output_filename, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            outfile.write(line.upper())\n",
        "\n",
        "    print(f'Converted {input_filename} to uppercase and saved as {output_filename}')"
      ],
      "metadata": {
        "id": "AJ0foNFde26N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q6. Convert all the files of Q4 into upper case parallel using multi-threading**"
      ],
      "metadata": {
        "id": "9Ra-2VAfggo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import os\n",
        "\n",
        "file_sizes_gb = [1, 2, 3, 4, 5]\n",
        "\n",
        "def convert_file_to_uppercase(index):\n",
        "    input_filename = f'random_file_{index}GB.txt'\n",
        "    output_filename = f'random_file_{index}GB_upper.txt'\n",
        "\n",
        "    with open(input_filename, 'r') as infile, open(output_filename, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            outfile.write(line.upper())\n",
        "    print(f'[Thread {index}] Converted {input_filename} to uppercase as {output_filename}')\n",
        "\n",
        "threads = []\n",
        "\n",
        "for i in range(1, len(file_sizes_gb) + 1):\n",
        "    t = threading.Thread(target=convert_file_to_uppercase, args=(i,))\n",
        "    threads.append(t)\n",
        "    t.start()\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"All files converted in parallel.\")"
      ],
      "metadata": {
        "id": "DhIU9RvWgh9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q7. WAP to automatically download 10 images of cat from “Google Images”.**"
      ],
      "metadata": {
        "id": "MU4CQTHagp8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler"
      ],
      "metadata": {
        "id": "fnpW9i3qg0vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "\n",
        "save_dir = '/content/cat_images'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "google_crawler = GoogleImageCrawler(storage={'root_dir': save_dir})\n",
        "google_crawler.crawl(keyword='cat', max_num=10)\n",
        "\n",
        "print(f'Images saved to: {save_dir}')"
      ],
      "metadata": {
        "id": "vbpXC5fS_YAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q8. WAP to automatically download 10 videos of “Machine Learning” from “Youtube.com”.**"
      ],
      "metadata": {
        "id": "vpAWVzNyd4UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U yt-dlp"
      ],
      "metadata": {
        "id": "p0RfeblweDeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yt_dlp\n",
        "\n",
        "save_path = \"/content/machine_learning_videos\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "search_query = \"ytsearch10:Machine Learning\"\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestvideo[ext=mp4][height<=1080]+bestaudio[ext=m4a]/best[ext=mp4][height<=1080]/best',\n",
        "    'outtmpl': os.path.join(save_path, '%(title).80s.%(ext)s'),\n",
        "    'merge_output_format': 'mp4',\n",
        "    'quiet': False,\n",
        "    'noplaylist': True,\n",
        "    'ignoreerrors': True\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([search_query])\n",
        "\n",
        "print(f\"\\n Download complete! Videos saved in: {save_path}\")"
      ],
      "metadata": {
        "id": "AFSSwwBWf29A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q9. Convert all the videos of Q8 and convert it to audio.**"
      ],
      "metadata": {
        "id": "DU35odPajwtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "GXHFBjV0kBnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "video_folder = \"/content/machine_learning_videos\"\n",
        "audio_folder = \"/content/machine_learning_audio\"\n",
        "os.makedirs(audio_folder, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(video_folder):\n",
        "    if filename.endswith(\".mp4\"):\n",
        "        video_path = os.path.join(video_folder, filename)\n",
        "        audio_filename = os.path.splitext(filename)[0] + \".mp3\"\n",
        "        audio_path = os.path.join(audio_folder, audio_filename)\n",
        "\n",
        "        try:\n",
        "            print(f\"Converting: {filename}\")\n",
        "            video_clip = VideoFileClip(video_path)\n",
        "            video_clip.audio.write_audiofile(audio_path, logger=None)\n",
        "            video_clip.close()\n",
        "            print(f\"Saved audio as: {audio_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to convert {filename}: {e}\")\n",
        "\n",
        "print(\"\\n All videos converted to audio!\")"
      ],
      "metadata": {
        "id": "9XPb-_AZkWgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q10. Create an automated pipeline using multi-threading for:**\n",
        "# **“Automatic Download of 100 Videos from YouTube” → “Convert it to Audio”.**"
      ],
      "metadata": {
        "id": "nlqP2Rqplyja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U yt-dlp\n",
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "iFqGv-YSq-ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "from moviepy.editor import VideoFileClip\n",
        "import yt_dlp\n",
        "\n",
        "video_dir = \"/content/youtube_100_videos\"\n",
        "audio_dir = \"/content/youtube_100_audios\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "os.makedirs(audio_dir, exist_ok=True)\n",
        "\n",
        "search_query = \"ytsearch100:Machine Learning\"\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestvideo[ext=mp4][height<=1080]+bestaudio[ext=m4a]/best[ext=mp4][height<=1080]/best',\n",
        "    'outtmpl': os.path.join(video_dir, '%(title).80s.%(ext)s'),\n",
        "    'merge_output_format': 'mp4',\n",
        "    'quiet': False,\n",
        "    'noplaylist': True,\n",
        "    'ignoreerrors': True\n",
        "}\n",
        "\n",
        "print(\"Downloading 100 videos...\")\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([search_query])\n",
        "\n",
        "print(\"Video download complete.\\n\")\n",
        "\n",
        "def convert_to_audio(video_path, audio_path):\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        clip.audio.write_audiofile(audio_path, logger=None)\n",
        "        clip.close()\n",
        "        print(f\"Converted: {os.path.basename(video_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {video_path}: {e}\")\n",
        "\n",
        "threads = []\n",
        "\n",
        "print(\"Starting audio conversion in threads...\\n\")\n",
        "for filename in os.listdir(video_dir):\n",
        "    if filename.endswith(\".mp4\"):\n",
        "        video_path = os.path.join(video_dir, filename)\n",
        "        audio_filename = os.path.splitext(filename)[0] + \".mp3\"\n",
        "        audio_path = os.path.join(audio_dir, audio_filename)\n",
        "\n",
        "        t = threading.Thread(target=convert_to_audio, args=(video_path, audio_path))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"\\n All videos converted to audio!\")"
      ],
      "metadata": {
        "id": "LxnCwGEkm8HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q11. Create an automated pipeline using multi-threading for: “Automatic Download of 500 images of Dog from GoogleImages” → “Rescale it to 50%”**"
      ],
      "metadata": {
        "id": "QOU9Ag45CcC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler pillow"
      ],
      "metadata": {
        "id": "FRRmjE2RwDcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "from PIL import Image\n",
        "import threading\n",
        "\n",
        "NUM_IMAGES = 2\n",
        "DOWNLOAD_DIR = 'dog_images'\n",
        "RESIZED_DIR = 'resized_dog_images'\n",
        "\n",
        "def download_images():\n",
        "    if not os.path.exists(DOWNLOAD_DIR):\n",
        "        os.makedirs(DOWNLOAD_DIR)\n",
        "\n",
        "    crawler = GoogleImageCrawler(storage={'root_dir': DOWNLOAD_DIR})\n",
        "    crawler.crawl(keyword='dog', max_num=NUM_IMAGES)\n",
        "\n",
        "def resize_image(image_path, output_dir):\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        new_size = (img.width // 2, img.height // 2)\n",
        "        img_resized = img.resize(new_size)\n",
        "        base_name = os.path.basename(image_path)\n",
        "        img_resized.save(os.path.join(output_dir, base_name))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to resize {image_path}: {e}\")\n",
        "\n",
        "def threaded_resize_images():\n",
        "    if not os.path.exists(RESIZED_DIR):\n",
        "        os.makedirs(RESIZED_DIR)\n",
        "\n",
        "    threads = []\n",
        "    for filename in os.listdir(DOWNLOAD_DIR):\n",
        "        path = os.path.join(DOWNLOAD_DIR, filename)\n",
        "        if os.path.isfile(path):\n",
        "            thread = threading.Thread(target=resize_image, args=(path, RESIZED_DIR))\n",
        "            thread.start()\n",
        "            threads.append(thread)\n",
        "\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Downloading images...\")\n",
        "    download_images()\n",
        "    print(\"Resizing images with multithreading...\")\n",
        "    threaded_resize_images()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "id": "qxuxrsHEghz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q12. Create a random dataset of 100 rows and 30 columns. All the values are defined between [1,200].**"
      ],
      "metadata": {
        "id": "EfUstifVjpoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    np.random.randint(1, 201, size=(100, 30)),\n",
        "    columns=[f'col_{i+1}' for i in range(30)]\n",
        ")\n",
        "\n",
        "df.to_csv(\"random_dataset.csv\", index=False)\n",
        "print(\"Dataset saved as 'random_dataset.csv'\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "WifokGe2j4rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform the following operations on the dataset created in Q12:**\n",
        "### (i) Replace all the values with NA in the dataset defined between [10, 60]. Print the count of number rows having missing values."
      ],
      "metadata": {
        "id": "uG25ClWIk0ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_masked = df.astype(object)\n",
        "df_masked = df_masked.apply(pd.to_numeric, errors='coerce')\n",
        "df_masked[(df_masked >= 10) & (df_masked <= 60)] = np.nan\n",
        "df_masked = df_masked.fillna('NA')\n",
        "\n",
        "df_masked.to_csv(\"updated_dataset_1.csv\", index=False)\n",
        "\n",
        "missing_rows_count = (df_masked == 'NA').any(axis=1).sum()\n",
        "\n",
        "print(\"Dataset saved as 'updated_dataset_1.csv'\")\n",
        "print(\"Number of rows with at least one 'NA':\", missing_rows_count)"
      ],
      "metadata": {
        "id": "imHZeJrilVzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (ii) Replace all the NA values with the average of the column value."
      ],
      "metadata": {
        "id": "hDRt5nfpl-t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_masked = pd.read_csv(\"updated_dataset_1.csv\")\n",
        "df_replaced = df_masked.replace('NA', np.nan)\n",
        "df_replaced = df_replaced.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "df_filled = df_replaced.fillna(df_replaced.mean(numeric_only=True))\n",
        "\n",
        "df_filled.to_csv(\"updated_dataset_2.csv\", index=False)\n",
        "\n",
        "print(\"Missing values replaced with column averages.\")\n",
        "print(\"Final dataset saved as 'updated_dataset_2.csv'\")"
      ],
      "metadata": {
        "id": "SItE3YZimD85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (iii) Find the Pearson correlation among all the columns and plot heat map. Also select those columns having correlation <=0.7."
      ],
      "metadata": {
        "id": "bCkEL8_jmzmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr_matrix = df_filled.corr(method='pearson')\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
        "plt.title(\"Pearson Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "low_corr_cols = [col for col in upper_triangle.columns if all(upper_triangle[col].abs().dropna() <= 0.7)]\n",
        "\n",
        "print(f\"\\nColumns with all correlations ≤ 0.7:\\n{low_corr_cols}\")"
      ],
      "metadata": {
        "id": "DJLn6f4Bm58K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (iv) Normalize all the values in the dataset between 0 and 10"
      ],
      "metadata": {
        "id": "j9NCTzCqnZan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_normalized = 10 * (df_filled - df_filled.min()) / (df_filled.max() - df_filled.min())\n",
        "\n",
        "df_normalized.to_csv(\"updated_dataset_3.csv\", index=False)\n",
        "print(\"Dataset saved as 'updated_dataset_3.csv'\")"
      ],
      "metadata": {
        "id": "AmCiw-mTncx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (v) Replace all the values in the dataset with 1 if value <=5 else with 0."
      ],
      "metadata": {
        "id": "QxXuZ9A0n6p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_binary = df_normalized.applymap(lambda x: 1 if x <= 5 else 0)\n",
        "\n",
        "df_binary.to_csv(\"updated_dataset_4.csv\", index=False)\n",
        "print(\"Dataset saved as 'updated_dataset_4.csv'\")"
      ],
      "metadata": {
        "id": "oLnqkumhoDNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q13. Create a random dataset of 500 rows and 10 columns.**\n",
        "## Columns 1 to 4 are defined between [-10, 10];\n",
        "## Columns 5 to 8 are defined between [10, 20];\n",
        "## Columns 9 to 10 are defined between [-100, 100]."
      ],
      "metadata": {
        "id": "8Dhjieb5hU62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "data_col_1_to_4 = np.random.randint(-10, 11, size=(500, 4))\n",
        "data_col_5_to_8 = np.random.randint(10, 21, size=(500, 4))\n",
        "data_col_9_to_10 = np.random.randint(-100, 101, size=(500, 2))\n",
        "\n",
        "all_data = np.concatenate((data_col_1_to_4, data_col_5_to_8, data_col_9_to_10), axis=1)\n",
        "column_names = [f'col_{i+1}' for i in range(10)]\n",
        "df = pd.DataFrame(all_data, columns=column_names)\n",
        "\n",
        "df.to_csv(\"random_dataset.csv\", index=False)\n",
        "print(\"Dataset saved as 'random_dataset.csv'\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "HJosBm_DhjGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply following clustering algorithms; determine the optimal number of clusters and plot distance metric graph using each algorithms:**\n"
      ],
      "metadata": {
        "id": "Li2IcZRCjT5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (i) K-Mean clustering"
      ],
      "metadata": {
        "id": "9r0-XQrkjhOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib seaborn scikit-learn scipy"
      ],
      "metadata": {
        "id": "YhV63J0mjk7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(K_range, inertia, marker='o')\n",
        "plt.title('K-Means Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia (Within-cluster SSE)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l37nkzt0kCKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (ii) Hierarchical clustering"
      ],
      "metadata": {
        "id": "Oj9l2YICker0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "linkage_matrix = linkage(scaled_data, method='ward')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(linkage_matrix, truncate_mode='lastp', p=20, leaf_rotation=45., leaf_font_size=12.)\n",
        "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
        "plt.xlabel('Cluster Size')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sarEbZonkZcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q14. Create a random dataset of 600 rows and 15 columns. All the values are defined between [-100,100]**"
      ],
      "metadata": {
        "id": "H0bjH_N4ktl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    np.random.randint(-100, 101, size=(600, 15)),\n",
        "    columns=[f'col_{i+1}' for i in range(15)]\n",
        ")\n",
        "\n",
        "df.to_csv(\"random_dataset.csv\", index=False)\n",
        "print(\"Dataset saved as 'random_dataset.csv'\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "qvcAomAqk0v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform the following operations:**"
      ],
      "metadata": {
        "id": "lCQIGLTUKggy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (i) Plot scatter graph between Column 5 and Column 6."
      ],
      "metadata": {
        "id": "-7BjcnD2Kk6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['col_5'], df['col_6'], alpha=0.6, color='teal', edgecolor='k')\n",
        "plt.title('Scatter Plot: Column 5 vs Column 6')\n",
        "plt.xlabel('col_5')\n",
        "plt.ylabel('col_6')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WPqQxzKVKxgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (ii) Plot histogram of each column in single graph."
      ],
      "metadata": {
        "id": "1c8c1OTuLEyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(18, 10))\n",
        "fig.suptitle('Histograms of All Columns', fontsize=16)\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(df.columns):\n",
        "    axes[i].hist(df[col], bins=20, color='skyblue', edgecolor='black')\n",
        "    axes[i].set_title(col)\n",
        "    axes[i].tick_params(axis='both', which='major', labelsize=8)\n",
        "\n",
        "for j in range(len(df.columns), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wdXQ_i_GLGrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (iii) Plot the Box plot of each column in single graph."
      ],
      "metadata": {
        "id": "AMeYY1z4Lkby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.boxplot(data=df, orient='v', palette='Set2')\n",
        "\n",
        "plt.title('Box Plot of All Columns', fontsize=16)\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Values')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KUm4kirGLppI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q15. Create a random dataset of 500 rows and 5 columns:**\n",
        "## All the values are defined between [5,10]."
      ],
      "metadata": {
        "id": "RbVavnxFL1Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    np.random.randint(5, 11, size=(500, 5)),\n",
        "    columns=[f'col_{i+1}' for i in range(5)]\n",
        ")\n",
        "\n",
        "df.to_csv(\"random_dataset.csv\", index=False)\n",
        "print(\"Dataset saved as 'random_dataset.csv'\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "cDLeva7cL_Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform the following operations:**"
      ],
      "metadata": {
        "id": "OEPg_9FMMPhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (i) Perform t-Test on each column."
      ],
      "metadata": {
        "id": "q4ez2YP4MTtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "population_mean = 7\n",
        "t_test_results = {}\n",
        "\n",
        "for col in df.columns:\n",
        "    t_stat, p_value = ttest_1samp(df[col], population_mean)\n",
        "    t_test_results[col] = {'t_statistic': t_stat, 'p_value': p_value}\n",
        "\n",
        "results_df = pd.DataFrame(t_test_results).T\n",
        "print(\"One-Sample t-Test Results (H0: mean == 7):\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "vfrEyjgVMTWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (ii) Perform Wilcoxon Signed Rank Test on each column"
      ],
      "metadata": {
        "id": "oJjzfS9eM3do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import wilcoxon\n",
        "\n",
        "reference_value = 7\n",
        "wilcoxon_results = {}\n",
        "\n",
        "for col in df.columns:\n",
        "    diffs = df[col] - reference_value\n",
        "    diffs_nonzero = diffs[diffs != 0]\n",
        "\n",
        "    if len(diffs_nonzero) > 0:\n",
        "        stat, p = wilcoxon(diffs_nonzero)\n",
        "        wilcoxon_results[col] = {'statistic': stat, 'p_value': p}\n",
        "    else:\n",
        "        wilcoxon_results[col] = {'statistic': np.nan, 'p_value': np.nan}\n",
        "\n",
        "results_df = pd.DataFrame(wilcoxon_results).T\n",
        "print(\"Wilcoxon Signed-Rank Test Results (H0: median == 7):\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "kpraGSfZM6XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4"
      ],
      "metadata": {
        "id": "UkezkxaHNlRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind, mannwhitneyu\n",
        "\n",
        "col3 = df['col_3']\n",
        "col4 = df['col_4']\n",
        "\n",
        "t_stat, t_p = ttest_ind(col3, col4, equal_var=False)\n",
        "u_stat, u_p = mannwhitneyu(col3, col4, alternative='two-sided')\n",
        "\n",
        "print(\"Two-Sample t-Test (col_3 vs col_4):\")\n",
        "print(f\"t-statistic = {t_stat:.4f}, p-value = {t_p:.4f}\\n\")\n",
        "\n",
        "print(\"Wilcoxon Rank-Sum Test (col_3 vs col_4):\")\n",
        "print(f\"U-statistic = {u_stat:.4f}, p-value = {u_p:.4f}\")"
      ],
      "metadata": {
        "id": "S9jxXUmDNo65"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}